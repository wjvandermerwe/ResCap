@article{polce_guide_2023,
	title = {A {Guide} for the {Application} of {Statistics} in {Biomedical} {Studies} {Concerning} {Machine} {Learning} and {Artificial} {Intelligence}},
	volume = {39},
	issn = {07498063},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0749806322002821},
	doi = {10.1016/j.arthro.2022.04.016},
	language = {en},
	number = {2},
	urldate = {2024-04-16},
	journal = {Arthroscopy: The Journal of Arthroscopic \& Related Surgery},
	author = {Polce, Evan M. and Kunze, Kyle N.},
	month = feb,
	year = {2023},
	pages = {151--158},
}

@article{tibshirani_regression_1996,
	title = {Regression {Shrinkage} and {Selection} via the {Lasso}},
	volume = {58},
	issn = {00359246},
	url = {http://www.jstor.org/stable/2346178},
	abstract = {[We propose a new method for estimation in linear models. The `lasso' minimizes the residual sum of squares subject to the sum of the absolute value of the coefficients being less than a constant. Because of the nature of this constraint it tends to produce some coefficients that are exactly 0 and hence gives interpretable models. Our simulation studies suggest that the lasso enjoys some of the favourable properties of both subset selection and ridge regression. It produces interpretable models like subset selection and exhibits the stability of ridge regression. There is also an interesting relationship with recent work in adaptive function estimation by Donoho and Johnstone. The lasso idea is quite general and can be applied in a variety of statistical models: extensions to generalized regression models and tree-based models are briefly described.]},
	number = {1},
	urldate = {2024-04-26},
	journal = {Journal of the Royal Statistical Society. Series B (Methodological)},
	author = {Tibshirani, Robert},
	year = {1996},
	note = {Publisher: [Royal Statistical Society, Wiley]},
	pages = {267--288},
}

@article{burzykowski_survival_2024,
	title = {Survival analysis: {Methods} for analyzing data with censored observations},
	volume = {30},
	issn = {10738746},
	shorttitle = {Survival analysis},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S1073874624000094},
	doi = {10.1053/j.sodo.2024.01.008},
	language = {en},
	number = {1},
	urldate = {2024-04-16},
	journal = {Seminars in Orthodontics},
	author = {Burzykowski, Tomasz},
	month = feb,
	year = {2024},
	pages = {29--36},
	file = {Full Text:/Users/johan/Zotero/storage/TWXT56GP/Burzykowski - 2024 - Survival analysis Methods for analyzing data with.pdf:application/pdf},
}

@article{pawel_pitfalls_2024,
	title = {Pitfalls and potentials in simulation studies: {Questionable} research practices in comparative simulation studies allow for spurious claims of superiority of any method},
	volume = {66},
	issn = {0323-3847, 1521-4036},
	shorttitle = {Pitfalls and potentials in simulation studies},
	url = {http://arxiv.org/abs/2203.13076},
	doi = {10.1002/bimj.202200091},
	abstract = {Comparative simulation studies are workhorse tools for benchmarking statistical methods. As with other empirical studies, the success of simulation studies hinges on the quality of their design, execution and reporting. If not conducted carefully and transparently, their conclusions may be misleading. In this paper we discuss various questionable research practices which may impact the validity of simulation studies, some of which cannot be detected or prevented by the current publication process in statistics journals. To illustrate our point, we invent a novel prediction method with no expected performance gain and benchmark it in a pre-registered comparative simulation study. We show how easy it is to make the method appear superior over well-established competitor methods if questionable research practices are employed. Finally, we provide concrete suggestions for researchers, reviewers and other academic stakeholders for improving the methodological quality of comparative simulation studies, such as pre-registering simulation protocols, incentivizing neutral simulation studies and code and data sharing.},
	number = {1},
	urldate = {2024-04-16},
	journal = {Biometrical Journal},
	author = {Pawel, Samuel and Kook, Lucas and Reeve, Kelly},
	month = jan,
	year = {2024},
	note = {arXiv:2203.13076 [stat]},
	keywords = {Statistics - Computation, Statistics - Methodology},
	pages = {2200091},
	file = {arXiv Fulltext PDF:/Users/johan/Zotero/storage/VIKVRGJI/Pawel et al. - 2024 - Pitfalls and potentials in simulation studies Que.pdf:application/pdf;arXiv.org Snapshot:/Users/johan/Zotero/storage/9Q9IYNRD/2203.html:text/html},
}

@article{smith_scoping_2022,
	title = {A scoping methodological review of simulation studies comparing statistical and machine learning approaches to risk prediction for time-to-event data},
	volume = {6},
	issn = {2397-7523},
	url = {https://diagnprognres.biomedcentral.com/articles/10.1186/s41512-022-00124-y},
	doi = {10.1186/s41512-022-00124-y},
	abstract = {Abstract
            
              Background
              There is substantial interest in the adaptation and application of so-called machine learning approaches to prognostic modelling of censored time-to-event data. These methods must be compared and evaluated against existing methods in a variety of scenarios to determine their predictive performance. A scoping review of how machine learning methods have been compared to traditional survival models is important to identify the comparisons that have been made and issues where they are lacking, biased towards one approach or misleading.
            
            
              Methods
              We conducted a scoping review of research articles published between 1 January 2000 and 2 December 2020 using PubMed. Eligible articles were those that used simulation studies to compare statistical and machine learning methods for risk prediction with a time-to-event outcome in a medical/healthcare setting. We focus on data-generating mechanisms (DGMs), the methods that have been compared, the estimands of the simulation studies, and the performance measures used to evaluate them.
            
            
              Results
              A total of ten articles were identified as eligible for the review. Six of the articles evaluated a method that was developed by the authors, four of which were machine learning methods, and the results almost always stated that this developed method’s performance was equivalent to or better than the other methods compared. Comparisons were often biased towards the novel approach, with the majority only comparing against a basic Cox proportional hazards model, and in scenarios where it is clear it would not perform well. In many of the articles reviewed, key information was unclear, such as the number of simulation repetitions and how performance measures were calculated.
            
            
              Conclusion
              It is vital that method comparisons are unbiased and comprehensive, and this should be the goal even if realising it is difficult. Fully assessing how newly developed methods perform and how they compare to a variety of traditional statistical methods for prognostic modelling is imperative as these methods are already being applied in clinical contexts. Evaluations of the performance and usefulness of recently developed methods for risk prediction should be continued and reporting standards improved as these methods become increasingly popular.},
	language = {en},
	number = {1},
	urldate = {2024-04-16},
	journal = {Diagnostic and Prognostic Research},
	author = {Smith, Hayley and Sweeting, Michael and Morris, Tim and Crowther, Michael J.},
	month = jun,
	year = {2022},
	pages = {10},
	file = {Full Text:/Users/johan/Zotero/storage/5236RNKE/Smith et al. - 2022 - A scoping methodological review of simulation stud.pdf:application/pdf},
}

@article{morris_using_2019,
	title = {Using simulation studies to evaluate statistical methods},
	volume = {38},
	issn = {0277-6715, 1097-0258},
	url = {http://arxiv.org/abs/1712.03198},
	doi = {10.1002/sim.8086},
	abstract = {Simulation studies are computer experiments that involve creating data by pseudorandom sampling. The key strength of simulation studies is the ability to understand the behaviour of statistical methods because some 'truth' (usually some parameter/s of interest) is known from the process of generating the data. This allows us to consider properties of methods, such as bias. While widely used, simulation studies are often poorly designed, analysed and reported. This tutorial outlines the rationale for using simulation studies and offers guidance for design, execution, analysis, reporting and presentation. In particular, this tutorial provides: a structured approach for planning and reporting simulation studies, which involves defining aims, data-generating mechanisms, estimands, methods and performance measures ('ADEMP'); coherent terminology for simulation studies; guidance on coding simulation studies; a critical discussion of key performance measures and their estimation; guidance on structuring tabular and graphical presentation of results; and new graphical presentations. With a view to describing recent practice, we review 100 articles taken from Volume 34 of Statistics in Medicine that included at least one simulation study and identify areas for improvement.},
	number = {11},
	urldate = {2024-04-16},
	journal = {Statistics in Medicine},
	author = {Morris, Tim P. and White, Ian R. and Crowther, Michael J.},
	month = may,
	year = {2019},
	note = {arXiv:1712.03198 [stat]},
	keywords = {Statistics - Methodology},
	pages = {2074--2102},
	annote = {Comment: 31 pages, 9 figures (2 in appendix), 8 tables (1 in appendix)},
	file = {arXiv Fulltext PDF:/Users/johan/Zotero/storage/JAW3BGRL/Morris et al. - 2019 - Using simulation studies to evaluate statistical m.pdf:application/pdf;arXiv.org Snapshot:/Users/johan/Zotero/storage/YIBPZQYM/1712.html:text/html},
}

@article{kurt_omurlu_comparisons_2009,
	title = {The comparisons of random survival forests and {Cox} regression analysis with simulation and an application related to breast cancer},
	volume = {36},
	copyright = {https://www.elsevier.com/tdm/userlicense/1.0/},
	issn = {09574174},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0957417408007343},
	doi = {10.1016/j.eswa.2008.10.023},
	language = {en},
	number = {4},
	urldate = {2024-04-16},
	journal = {Expert Systems with Applications},
	author = {Kurt Omurlu, Imran and Ture, Mevlut and Tokatli, Füsun},
	month = may,
	year = {2009},
	pages = {8582--8588},
}

@article{kantidakis_simulation_2021,
	title = {A {Simulation} {Study} to {Compare} the {Predictive} {Performance} of {Survival} {Neural} {Networks} with {Cox} {Models} for {Clinical} {Trial} {Data}},
	volume = {2021},
	copyright = {https://creativecommons.org/licenses/by/4.0/},
	issn = {1748-6718, 1748-670X},
	url = {https://www.hindawi.com/journals/cmmm/2021/2160322/},
	doi = {10.1155/2021/2160322},
	abstract = {Background. Studies focusing on prediction models are widespread in medicine. There is a trend in applying machine learning (ML) by medical researchers and clinicians. Over the years, multiple ML algorithms have been adapted to censored data. However, the choice of methodology should be motivated by the real-life data and their complexity. Here, the predictive performance of ML techniques is compared with statistical models in a simple clinical setting (small/moderate sample size and small number of predictors) with Monte-Carlo simulations. Methods. Synthetic data (250 or 1000 patients) were generated that closely resembled 5 prognostic factors preselected based on a European Osteosarcoma Intergroup study (MRC BO06/EORTC 80931). Comparison was performed between 2 partial logistic artificial neural networks (PLANNs) and Cox models for 20, 40, 61, and 80\% censoring. Survival times were generated from a log-normal distribution. Models were contrasted in terms of the C-index, Brier score at 0-5 years, integrated Brier score (IBS) at 5 years, and miscalibration at 2 and 5 years (usually neglected). The endpoint of interest was overall survival. Results. PLANNs original/extended were tuned based on the IBS at 5 years and the C-index, achieving a slightly better performance with the IBS. Comparison with Cox models showed that PLANNs can reach similar predictive performance on simulated data for most scenarios with respect to the C-index, Brier score, or IBS. However, Cox models were frequently less miscalibrated. Performance was robust in scenario data where censored patients were removed before 2 years or curtailing at 5 years was performed (on training data). Conclusion. Survival neural networks reached a comparable predictive performance with Cox models but were generally less well calibrated. All in all, researchers should be aware of burdensome aspects of ML techniques such as data preprocessing, tuning of hyperparameters, and computational intensity that render them disadvantageous against conventional regression models in a simple clinical setting.},
	language = {en},
	urldate = {2024-04-16},
	journal = {Computational and Mathematical Methods in Medicine},
	author = {Kantidakis, Georgios and Biganzoli, Elia and Putter, Hein and Fiocco, Marta},
	editor = {Bursac, Zoran},
	month = nov,
	year = {2021},
	pages = {1--15},
	file = {Full Text:/Users/johan/Zotero/storage/68AAIM2T/Kantidakis et al. - 2021 - A Simulation Study to Compare the Predictive Perfo.pdf:application/pdf},
}

@misc{fan_high-dimensional_2010,
	title = {High-dimensional variable selection for {Cox}'s proportional hazards model},
	url = {http://arxiv.org/abs/1002.3315},
	abstract = {Variable selection in high dimensional space has challenged many contemporary statistical problems from many frontiers of scientific disciplines. Recent technology advance has made it possible to collect a huge amount of covariate information such as microarray, proteomic and SNP data via bioimaging technology while observing survival information on patients in clinical studies. Thus, the same challenge applies to the survival analysis in order to understand the association between genomics information and clinical information about the survival time. In this work, we extend the sure screening procedure Fan and Lv (2008) to Cox's proportional hazards model with an iterative version available. Numerical simulation studies have shown encouraging performance of the proposed method in comparison with other techniques such as LASSO. This demonstrates the utility and versatility of the iterative sure independent screening scheme.},
	urldate = {2024-04-16},
	publisher = {arXiv},
	author = {Fan, Jianqing and Feng, Yang and Wu, Yichao},
	month = may,
	year = {2010},
	note = {arXiv:1002.3315 [stat]},
	keywords = {Statistics - Methodology, Statistics - Machine Learning},
	annote = {Comment: 17 pages, 5 figures},
	file = {arXiv Fulltext PDF:/Users/johan/Zotero/storage/2LPWIJV7/Fan et al. - 2010 - High-dimensional variable selection for Cox's prop.pdf:application/pdf;arXiv.org Snapshot:/Users/johan/Zotero/storage/5UKQI4MG/1002.html:text/html},
}

@misc{ahrens_lassopack_2019,
	title = {lassopack: {Model} selection and prediction with regularized regression in {Stata}},
	shorttitle = {lassopack},
	url = {http://arxiv.org/abs/1901.05397},
	abstract = {This article introduces lassopack, a suite of programs for regularized regression in Stata. lassopack implements lasso, square-root lasso, elastic net, ridge regression, adaptive lasso and post-estimation OLS. The methods are suitable for the high-dimensional setting where the number of predictors \$p\$ may be large and possibly greater than the number of observations, \$n\$. We offer three different approaches for selecting the penalization (`tuning') parameters: information criteria (implemented in lasso2), \$K\$-fold cross-validation and \$h\$-step ahead rolling cross-validation for cross-section, panel and time-series data (cvlasso), and theory-driven (`rigorous') penalization for the lasso and square-root lasso for cross-section and panel data (rlasso). We discuss the theoretical framework and practical considerations for each approach. We also present Monte Carlo results to compare the performance of the penalization approaches.},
	urldate = {2024-04-16},
	publisher = {arXiv},
	author = {Ahrens, Achim and Hansen, Christian B. and Schaffer, Mark E.},
	month = jan,
	year = {2019},
	note = {arXiv:1901.05397 [econ]},
	keywords = {Economics - Econometrics},
	annote = {Comment: 52 pages, 6 figures, 6 tables; submitted to Stata Journal; for more information see https://statalasso.github.io/},
	file = {arXiv Fulltext PDF:/Users/johan/Zotero/storage/63STH4ER/Ahrens et al. - 2019 - lassopack Model selection and prediction with reg.pdf:application/pdf;arXiv.org Snapshot:/Users/johan/Zotero/storage/M4YCWTJH/1901.html:text/html},
}

@misc{andreux_federated_2020,
	title = {Federated {Survival} {Analysis} with {Discrete}-{Time} {Cox} {Models}},
	url = {http://arxiv.org/abs/2006.08997},
	abstract = {Building machine learning models from decentralized datasets located in different centers with federated learning (FL) is a promising approach to circumvent local data scarcity while preserving privacy. However, the prominent Cox proportional hazards (PH) model, used for survival analysis, does not fit the FL framework, as its loss function is non-separable with respect to the samples. The na{\textbackslash}"ive method to bypass this non-separability consists in calculating the losses per center, and minimizing their sum as an approximation of the true loss. We show that the resulting model may suffer from important performance loss in some adverse settings. Instead, we leverage the discrete-time extension of the Cox PH model to formulate survival analysis as a classification problem with a separable loss function. Using this approach, we train survival models using standard FL techniques on synthetic data, as well as real-world datasets from The Cancer Genome Atlas (TCGA), showing similar performance to a Cox PH model trained on aggregated data. Compared to previous works, the proposed method is more communication-efficient, more generic, and more amenable to using privacy-preserving techniques.},
	urldate = {2024-04-16},
	publisher = {arXiv},
	author = {Andreux, Mathieu and Manoel, Andre and Menuet, Romuald and Saillard, Charlie and Simpson, Chloé},
	month = jun,
	year = {2020},
	note = {arXiv:2006.08997 [cs, stat]},
	keywords = {Statistics - Machine Learning, Computer Science - Machine Learning},
	annote = {Comment: 21 pages, 6 figures},
	file = {arXiv Fulltext PDF:/Users/johan/Zotero/storage/PUKTUZM5/Andreux et al. - 2020 - Federated Survival Analysis with Discrete-Time Cox.pdf:application/pdf;arXiv.org Snapshot:/Users/johan/Zotero/storage/9W72JWLC/2006.html:text/html},
}

@misc{cygu_pcoxtime_2021,
	title = {pcoxtime: {Penalized} {Cox} {Proportional} {Hazard} {Model} for {Time}-dependent {Covariates}},
	shorttitle = {pcoxtime},
	url = {http://arxiv.org/abs/2102.02297},
	abstract = {The penalized Cox proportional hazard model is a popular analytical approach for survival data with a large number of covariates. Such problems are especially challenging when covariates vary over follow-up time (i.e., the covariates are time-dependent). The standard R packages for fully penalized Cox models cannot currently incorporate time-dependent covariates. To address this gap, we implement a variant of gradient descent algorithm (proximal gradient descent) for fitting penalized Cox models. We apply our implementation to real and simulated data sets.},
	urldate = {2024-04-16},
	publisher = {arXiv},
	author = {Cygu, Steve and Dushoff, Jonathan and Bolker, Benjamin M.},
	month = jun,
	year = {2021},
	note = {arXiv:2102.02297 [stat]},
	keywords = {Statistics - Computation, Statistics - Methodology, 14J60},
	annote = {Comment: Submitted to Journal of Statistical Software},
	file = {arXiv Fulltext PDF:/Users/johan/Zotero/storage/DEGX7LEL/Cygu et al. - 2021 - pcoxtime Penalized Cox Proportional Hazard Model .pdf:application/pdf;arXiv.org Snapshot:/Users/johan/Zotero/storage/6M5XGNTM/2102.html:text/html},
}

@article{cox_regression_1972,
	title = {Regression {Models} and {Life}-{Tables}},
	volume = {34},
	issn = {00359246},
	url = {http://www.jstor.org/stable/2985181},
	abstract = {[The analysis of censored failure times is considered. It is assumed that on each individual are available values of one or more explanatory variables. The hazard function (age-specific failure rate) is taken to be a function of the explanatory variables and unknown regression coefficients multiplied by an arbitrary and unknown function of time. A conditional likelihood is obtained, leading to inferences about the unknown regression coefficients. Some generalizations are outlined.]},
	number = {2},
	urldate = {2024-04-16},
	journal = {Journal of the Royal Statistical Society. Series B (Methodological)},
	author = {Cox, D. R.},
	year = {1972},
	note = {Publisher: [Royal Statistical Society, Wiley]},
	pages = {187--220},
}

@article{zhang_adaptive_2007,
	title = {Adaptive {Lasso} for {Cox}'s {Proportional} {Hazards} {Model}},
	volume = {94},
	issn = {00063444, 14643510},
	url = {http://www.jstor.org/stable/20441405},
	abstract = {[We investigate the variable selection problem for Cox's proportional hazards model, and propose a unified model selection and estimation procedure with desired theoretical properties and computational convenience. The new method is based on a penalized log partial likelihood with the adaptively weighted L₁ penalty on regression coefficients, providing what we call the adaptive Lasso estimator. The method incorporates different penalties for different coefficients: unimportant variables receive larger penalties than important ones, so that important variables tend to be retained in the selection process, whereas unimportant variables are more likely to be dropped. Theoretical properties, such as consistency and rate of convergence of the estimator, are studied. We also show that, with proper choice of regularization parameters, the proposed estimator has the oracle properties. The convex optimization nature of the method leads to an efficient algorithm. Both simulated and real examples show that the method performs competitively.]},
	number = {3},
	urldate = {2024-04-16},
	journal = {Biometrika},
	author = {Zhang, Hao Helen and Lu, Wenbin},
	year = {2007},
	note = {Publisher: [Oxford University Press, Biometrika Trust]},
	pages = {691--703},
}

@misc{yao_ensemble_2022,
	title = {Ensemble methods for survival function estimation with time-varying covariates},
	url = {http://arxiv.org/abs/2006.00567},
	abstract = {Survival data with time-varying covariates are common in practice. If relevant, they can improve on the estimation of survival function. However, the traditional survival forests - conditional inference forest, relative risk forest and random survival forest - have accommodated only time-invariant covariates. We generalize the conditional inference and relative risk forests to allow time-varying covariates. We also propose a general framework for estimation of a survival function in the presence of time-varying covariates. We compare their performance with that of the Cox model and transformation forest, adapted here to accommodate time-varying covariates, through a comprehensive simulation study in which the Kaplan-Meier estimate serves as a benchmark, and performance is compared using the integrated L2 difference between the true and estimated survival functions. In general, the performance of the two proposed forests substantially improves over the Kaplan-Meier estimate. Taking into account all other factors, under the proportional hazard (PH) setting, the best method is always one of the two proposed forests, while under the non-PH setting, it is the adapted transformation forest. K-fold cross-validation is used as an effective tool to choose between the methods in practice.},
	urldate = {2024-04-16},
	publisher = {arXiv},
	author = {Yao, Weichi and Frydman, Halina and Larocque, Denis and Simonoff, Jeffrey S.},
	month = jun,
	year = {2022},
	note = {arXiv:2006.00567 [stat]},
	keywords = {Statistics - Methodology, Statistics - Machine Learning, Statistics - Applications},
	file = {arXiv Fulltext PDF:/Users/johan/Zotero/storage/VMYVSKR9/Yao et al. - 2022 - Ensemble methods for survival function estimation .pdf:application/pdf;arXiv.org Snapshot:/Users/johan/Zotero/storage/TPULP75U/2006.html:text/html},
}

@article{shortreed_outcome-adaptive_2017,
	title = {Outcome-{Adaptive} {Lasso}: {Variable} {Selection} for {Causal} {Inference}},
	volume = {73},
	copyright = {https://academic.oup.com/journals/pages/open\_access/funder\_policies/chorus/standard\_publication\_model},
	issn = {0006-341X, 1541-0420},
	shorttitle = {Outcome-{Adaptive} {Lasso}},
	url = {https://academic.oup.com/biometrics/article/73/4/1111-1122/7537777},
	doi = {10.1111/biom.12679},
	abstract = {Summary
            Methodological advancements, including propensity score methods, have resulted in improved unbiased estimation of treatment effects from observational data. Traditionally, a “throw in the kitchen sink” approach has been used to select covariates for inclusion into the propensity score, but recent work shows including unnecessary covariates can impact both the bias and statistical efficiency of propensity score estimators. In particular, the inclusion of covariates that impact exposure but not the outcome, can inflate standard errors without improving bias, while the inclusion of covariates associated with the outcome but unrelated to exposure can improve precision. We propose the outcome-adaptive lasso for selecting appropriate covariates for inclusion in propensity score models to account for confounding bias and maintaining statistical efficiency. This proposed approach can perform variable selection in the presence of a large number of spurious covariates, that is, covariates unrelated to outcome or exposure. We present theoretical and simulation results indicating that the outcome-adaptive lasso selects the propensity score model that includes all true confounders and predictors of outcome, while excluding other covariates. We illustrate covariate selection using the outcome-adaptive lasso, including comparison to alternative approaches, using simulated data and in a survey of patients using opioid therapy to manage chronic pain.},
	language = {en},
	number = {4},
	urldate = {2024-04-16},
	journal = {Biometrics},
	author = {Shortreed, Susan M. and Ertefaie, Ashkan},
	month = dec,
	year = {2017},
	pages = {1111--1122},
	file = {Accepted Version:/Users/johan/Zotero/storage/E32SCUKX/Shortreed and Ertefaie - 2017 - Outcome-Adaptive Lasso Variable Selection for Cau.pdf:application/pdf},
}

@misc{jaeger_accelerated_2022,
	title = {Accelerated and interpretable oblique random survival forests},
	url = {http://arxiv.org/abs/2208.01129},
	abstract = {The oblique random survival forest (RSF) is an ensemble supervised learning method for right-censored outcomes. Trees in the oblique RSF are grown using linear combinations of predictors to create branches, whereas in the standard RSF, a single predictor is used. Oblique RSF ensembles often have higher prediction accuracy than standard RSF ensembles. However, assessing all possible linear combinations of predictors induces significant computational overhead that limits applications to large-scale data sets. In addition, few methods have been developed for interpretation of oblique RSF ensembles, and they remain more difficult to interpret compared to their axis-based counterparts. We introduce a method to increase computational efficiency of the oblique RSF and a method to estimate importance of individual predictor variables with the oblique RSF. Our strategy to reduce computational overhead makes use of Newton-Raphson scoring, a classical optimization technique that we apply to the Cox partial likelihood function within each non-leaf node of decision trees. We estimate the importance of individual predictors for the oblique RSF by negating each coefficient used for the given predictor in linear combinations, and then computing the reduction in out-of-bag accuracy. In general benchmarking experiments, we find that our implementation of the oblique RSF is approximately 450 times faster with equivalent discrimination and superior Brier score compared to existing software for oblique RSFs. We find in simulation studies that 'negation importance' discriminates between relevant and irrelevant predictors more reliably than permutation importance, Shapley additive explanations, and a previously introduced technique to measure variable importance with oblique RSFs based on analysis of variance. Methods introduced in the current study are available in the aorsf R package.},
	urldate = {2024-04-16},
	publisher = {arXiv},
	author = {Jaeger, Byron C. and Welden, Sawyer and Lenoir, Kristin and Speiser, Jaime L. and Segar, Matthew W. and Pandey, Ambarish and Pajewski, Nicholas M.},
	month = aug,
	year = {2022},
	note = {arXiv:2208.01129 [stat]},
	keywords = {Statistics - Methodology, Statistics - Machine Learning},
	annote = {Comment: 40 pages, 6 figures},
	file = {arXiv Fulltext PDF:/Users/johan/Zotero/storage/9TUTVJFH/Jaeger et al. - 2022 - Accelerated and interpretable oblique random survi.pdf:application/pdf;arXiv.org Snapshot:/Users/johan/Zotero/storage/M3YRVGUH/2208.html:text/html},
}

@inproceedings{archetti_federated_2023,
	title = {Federated {Survival} {Forests}},
	url = {http://arxiv.org/abs/2302.02807},
	doi = {10.1109/IJCNN54540.2023.10190999},
	abstract = {Survival analysis is a subfield of statistics concerned with modeling the occurrence time of a particular event of interest for a population. Survival analysis found widespread applications in healthcare, engineering, and social sciences. However, real-world applications involve survival datasets that are distributed, incomplete, censored, and confidential. In this context, federated learning can tremendously improve the performance of survival analysis applications. Federated learning provides a set of privacy-preserving techniques to jointly train machine learning models on multiple datasets without compromising user privacy, leading to a better generalization performance. However, despite the widespread development of federated learning in recent AI research, few studies focus on federated survival analysis. In this work, we present a novel federated algorithm for survival analysis based on one of the most successful survival models, the random survival forest. We call the proposed method Federated Survival Forest (FedSurF). With a single communication round, FedSurF obtains a discriminative power comparable to deep-learning-based federated models trained over hundreds of federated iterations. Moreover, FedSurF retains all the advantages of random forests, namely low computational cost and natural handling of missing values and incomplete datasets. These advantages are especially desirable in real-world federated environments with multiple small datasets stored on devices with low computational capabilities. Numerical experiments compare FedSurF with state-of-the-art survival models in federated networks, showing how FedSurF outperforms deep-learning-based federated algorithms in realistic environments with non-identically distributed data.},
	urldate = {2024-04-16},
	booktitle = {2023 {International} {Joint} {Conference} on {Neural} {Networks} ({IJCNN})},
	author = {Archetti, Alberto and Matteucci, Matteo},
	month = jun,
	year = {2023},
	note = {arXiv:2302.02807 [cs]},
	keywords = {Computer Science - Machine Learning},
	pages = {1--9},
	file = {arXiv Fulltext PDF:/Users/johan/Zotero/storage/XZUNPRX2/Archetti and Matteucci - 2023 - Federated Survival Forests.pdf:application/pdf;arXiv.org Snapshot:/Users/johan/Zotero/storage/6IGW9YQM/2302.html:text/html},
}

@article{ishwaran_random_2008,
	title = {Random survival forests},
	volume = {2},
	issn = {1932-6157},
	url = {http://arxiv.org/abs/0811.1645},
	doi = {10.1214/08-AOAS169},
	abstract = {We introduce random survival forests, a random forests method for the analysis of right-censored survival data. New survival splitting rules for growing survival trees are introduced, as is a new missing data algorithm for imputing missing data. A conservation-of-events principle for survival forests is introduced and used to define ensemble mortality, a simple interpretable measure of mortality that can be used as a predicted outcome. Several illustrative examples are given, including a case study of the prognostic implications of body mass for individuals with coronary artery disease. Computations for all examples were implemented using the freely available R-software package, randomSurvivalForest.},
	number = {3},
	urldate = {2024-04-16},
	journal = {The Annals of Applied Statistics},
	author = {Ishwaran, Hemant and Kogalur, Udaya B. and Blackstone, Eugene H. and Lauer, Michael S.},
	month = sep,
	year = {2008},
	note = {arXiv:0811.1645 [stat]},
	keywords = {Statistics - Applications},
	annote = {Comment: Published in at http://dx.doi.org/10.1214/08-AOAS169 the Annals of Applied Statistics (http://www.imstat.org/aoas/) by the Institute of Mathematical Statistics (http://www.imstat.org)},
	file = {arXiv Fulltext PDF:/Users/johan/Zotero/storage/3WQJ6PZB/Ishwaran et al. - 2008 - Random survival forests.pdf:application/pdf;arXiv.org Snapshot:/Users/johan/Zotero/storage/F24NGKM8/0811.html:text/html},
}

@article{jin_imputation_2024,
	title = {Imputation methods for informative censoring in survival analysis with time dependent covariates},
	volume = {136},
	issn = {15517144},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S1551714423003245},
	doi = {10.1016/j.cct.2023.107401},
	language = {en},
	urldate = {2024-04-16},
	journal = {Contemporary Clinical Trials},
	author = {Jin, Man},
	month = jan,
	year = {2024},
	pages = {107401},
}

@misc{norcliffe_survivalgan_2023,
	title = {{SurvivalGAN}: {Generating} {Time}-to-{Event} {Data} for {Survival} {Analysis}},
	shorttitle = {{SurvivalGAN}},
	url = {http://arxiv.org/abs/2302.12749},
	abstract = {Synthetic data is becoming an increasingly promising technology, and successful applications can improve privacy, fairness, and data democratization. While there are many methods for generating synthetic tabular data, the task remains non-trivial and unexplored for specific scenarios. One such scenario is survival data. Here, the key difficulty is censoring: for some instances, we are not aware of the time of event, or if one even occurred. Imbalances in censoring and time horizons cause generative models to experience three new failure modes specific to survival analysis: (1) generating too few at-risk members; (2) generating too many at-risk members; and (3) censoring too early. We formalize these failure modes and provide three new generative metrics to quantify them. Following this, we propose SurvivalGAN, a generative model that handles survival data firstly by addressing the imbalance in the censoring and event horizons, and secondly by using a dedicated mechanism for approximating time-to-event/censoring. We evaluate this method via extensive experiments on medical datasets. SurvivalGAN outperforms multiple baselines at generating survival data, and in particular addresses the failure modes as measured by the new metrics, in addition to improving downstream performance of survival models trained on the synthetic data.},
	urldate = {2024-04-16},
	publisher = {arXiv},
	author = {Norcliffe, Alexander and Cebere, Bogdan and Imrie, Fergus and Lio, Pietro and van der Schaar, Mihaela},
	month = feb,
	year = {2023},
	note = {arXiv:2302.12749 [cs]},
	keywords = {Computer Science - Machine Learning},
	file = {arXiv Fulltext PDF:/Users/johan/Zotero/storage/UUT8BXPL/Norcliffe et al. - 2023 - SurvivalGAN Generating Time-to-Event Data for Sur.pdf:application/pdf;arXiv.org Snapshot:/Users/johan/Zotero/storage/GAAHRMYM/2302.html:text/html},
}

@misc{thurow_how_2023,
	title = {How to {Simulate} {Realistic} {Survival} {Data}? {A} {Simulation} {Study} to {Compare} {Realistic} {Simulation} {Models}},
	shorttitle = {How to {Simulate} {Realistic} {Survival} {Data}?},
	url = {http://arxiv.org/abs/2308.07842},
	abstract = {In statistics, it is important to have realistic data sets available for a particular context to allow an appropriate and objective method comparison. For many use cases, benchmark data sets for method comparison are already available online. However, in most medical applications and especially for clinical trials in oncology, there is a lack of adequate benchmark data sets, as patient data can be sensitive and therefore cannot be published. A potential solution for this are simulation studies. However, it is sometimes not clear, which simulation models are suitable for generating realistic data. A challenge is that potentially unrealistic assumptions have to be made about the distributions. Our approach is to use reconstructed benchmark data sets \%can be used as a basis for the simulations, which has the following advantages: the actual properties are known and more realistic data can be simulated. There are several possibilities to simulate realistic data from benchmark data sets. We investigate simulation models based upon kernel density estimation, fitted distributions, case resampling and conditional bootstrapping. In order to make recommendations on which models are best suited for a specific survival setting, we conducted a comparative simulation study. Since it is not possible to provide recommendations for all possible survival settings in a single paper, we focus on providing realistic simulation models for two-armed phase III lung cancer studies. To this end we reconstructed benchmark data sets from recent studies. We used the runtime and different accuracy measures (effect sizes and p-values) as criteria for comparison.},
	urldate = {2024-04-16},
	publisher = {arXiv},
	author = {Thurow, Maria and Dormuth, Ina and Sauer, Christina and Ditzhaus, Marc and Pauly, Markus},
	month = aug,
	year = {2023},
	note = {arXiv:2308.07842 [stat]},
	keywords = {Statistics - Applications},
	file = {arXiv Fulltext PDF:/Users/johan/Zotero/storage/YHL8YGUN/Thurow et al. - 2023 - How to Simulate Realistic Survival Data A Simulat.pdf:application/pdf;arXiv.org Snapshot:/Users/johan/Zotero/storage/LVMI28MG/2308.html:text/html},
}

@article{meng_simulating_2023,
	title = {Simulating time-to-event data subject to competing risks and clustering: {A} review and synthesis},
	volume = {32},
	issn = {0962-2802, 1477-0334},
	shorttitle = {Simulating time-to-event data subject to competing risks and clustering},
	url = {http://journals.sagepub.com/doi/10.1177/09622802221136067},
	doi = {10.1177/09622802221136067},
	abstract = {Simulation studies play an important role in evaluating the performance of statistical models developed for analyzing complex survival data such as those with competing risks and clustering. This article aims to provide researchers with a basic understanding of competing risks data generation, techniques for inducing cluster-level correlation, and ways to combine them together in simulation studies, in the context of randomized clinical trials with a binary exposure or treatment. We review data generation with competing and semi-competing risks and three approaches of inducing cluster-level correlation for time-to-event data: the frailty model framework, the probability transform, and Moran’s algorithm. Using exponentially distributed event times as an example, we discuss how to introduce cluster-level correlation into generating complex survival outcomes, and illustrate multiple ways of combining these methods to simulate clustered, competing and semi-competing risks data with pre-specified correlation values or degree of clustering.},
	language = {en},
	number = {2},
	urldate = {2024-04-16},
	journal = {Statistical Methods in Medical Research},
	author = {Meng, Can and Esserman, Denise and Li, Fan and Zhao, Yize and Blaha, Ondrej and Lu, Wenhan and Wang, Yuxuan and Peduzzi, Peter and Greene, Erich J.},
	month = feb,
	year = {2023},
	pages = {305--333},
}

@book{pham_springer_2023,
	address = {London},
	series = {Springer {Handbooks}},
	title = {Springer {Handbook} of {Engineering} {Statistics}},
	copyright = {https://www.springernature.com/gp/researchers/text-and-data-mining},
	isbn = {978-1-4471-7502-5 978-1-4471-7503-2},
	url = {https://link.springer.com/10.1007/978-1-4471-7503-2},
	language = {en},
	urldate = {2024-04-16},
	publisher = {Springer London},
	editor = {Pham, Hoang},
	year = {2023},
	doi = {10.1007/978-1-4471-7503-2},
	file = {Submitted Version:/Users/johan/Zotero/storage/UZTDRQIU/Pham - 2023 - Springer Handbook of Engineering Statistics.pdf:application/pdf},
}

@misc{nagpal_auton-survival_2022,
	title = {auton-survival: an {Open}-{Source} {Package} for {Regression}, {Counterfactual} {Estimation}, {Evaluation} and {Phenotyping} with {Censored} {Time}-to-{Event} {Data}},
	shorttitle = {auton-survival},
	url = {http://arxiv.org/abs/2204.07276},
	abstract = {Applications of machine learning in healthcare often require working with time-to-event prediction tasks including prognostication of an adverse event, re-hospitalization or death. Such outcomes are typically subject to censoring due to loss of follow up. Standard machine learning methods cannot be applied in a straightforward manner to datasets with censored outcomes. In this paper, we present auton-survival, an open-source repository of tools to streamline working with censored time-to-event or survival data. auton-survival includes tools for survival regression, adjustment in the presence of domain shift, counterfactual estimation, phenotyping for risk stratification, evaluation, as well as estimation of treatment effects. Through real world case studies employing a large subset of the SEER oncology incidence data, we demonstrate the ability of auton-survival to rapidly support data scientists in answering complex health and epidemiological questions.},
	urldate = {2024-04-16},
	publisher = {arXiv},
	author = {Nagpal, Chirag and Potosnak, Willa and Dubrawski, Artur},
	month = aug,
	year = {2022},
	note = {arXiv:2204.07276 [cs, stat]},
	keywords = {Statistics - Machine Learning, Computer Science - Machine Learning, Computer Science - Mathematical Software},
	file = {arXiv Fulltext PDF:/Users/johan/Zotero/storage/CG83JYFS/Nagpal et al. - 2022 - auton-survival an Open-Source Package for Regress.pdf:application/pdf;arXiv.org Snapshot:/Users/johan/Zotero/storage/YRVFCS2X/2204.html:text/html},
}

@misc{sonabend_flexible_2022,
	title = {Flexible {Group} {Fairness} {Metrics} for {Survival} {Analysis}},
	url = {http://arxiv.org/abs/2206.03256},
	abstract = {Algorithmic fairness is an increasingly important field concerned with detecting and mitigating biases in machine learning models. There has been a wealth of literature for algorithmic fairness in regression and classification however there has been little exploration of the field for survival analysis. Survival analysis is the prediction task in which one attempts to predict the probability of an event occurring over time. Survival predictions are particularly important in sensitive settings such as when utilising machine learning for diagnosis and prognosis of patients. In this paper we explore how to utilise existing survival metrics to measure bias with group fairness metrics. We explore this in an empirical experiment with 29 survival datasets and 8 measures. We find that measures of discrimination are able to capture bias well whereas there is less clarity with measures of calibration and scoring rules. We suggest further areas for research including prediction-based fairness metrics for distribution predictions.},
	urldate = {2024-04-16},
	publisher = {arXiv},
	author = {Sonabend, Raphael and Pfisterer, Florian and Mishler, Alan and Schauer, Moritz and Burk, Lukas and Mukherjee, Sumantrak and Vollmer, Sebastian},
	month = jul,
	year = {2022},
	note = {arXiv:2206.03256 [cs, stat]},
	keywords = {Statistics - Methodology, Statistics - Applications, Computer Science - Machine Learning, Computer Science - Computers and Society},
	annote = {Comment: Accepted in DSHealth 2022 (Workshop on Applied Data Science for Healthcare)},
	file = {arXiv Fulltext PDF:/Users/johan/Zotero/storage/YWNJCA3L/Sonabend et al. - 2022 - Flexible Group Fairness Metrics for Survival Analy.pdf:application/pdf;arXiv.org Snapshot:/Users/johan/Zotero/storage/5YX77H5S/2206.html:text/html},
}

@misc{yanagisawa_proper_2023,
	title = {Proper {Scoring} {Rules} for {Survival} {Analysis}},
	url = {http://arxiv.org/abs/2305.00621},
	abstract = {Survival analysis is the problem of estimating probability distributions for future event times, which can be seen as a problem in uncertainty quantification. Although there are fundamental theories on strictly proper scoring rules for uncertainty quantification, little is known about those for survival analysis. In this paper, we investigate extensions of four major strictly proper scoring rules for survival analysis and we prove that these extensions are proper under certain conditions, which arise from the discretization of the estimation of probability distributions. We also compare the estimation performances of these extended scoring rules by using real datasets, and the extensions of the logarithmic score and the Brier score performed the best.},
	urldate = {2024-04-16},
	publisher = {arXiv},
	author = {Yanagisawa, Hiroki},
	month = jun,
	year = {2023},
	note = {arXiv:2305.00621 [cs, stat]},
	keywords = {Statistics - Methodology, Computer Science - Machine Learning},
	annote = {Comment: Accepted at ICML 2023},
	file = {arXiv Fulltext PDF:/Users/johan/Zotero/storage/WBU5D4Z9/Yanagisawa - 2023 - Proper Scoring Rules for Survival Analysis.pdf:application/pdf;arXiv.org Snapshot:/Users/johan/Zotero/storage/PIZWDFCE/2305.html:text/html},
}

@misc{qi_effective_2023,
	title = {An {Effective} {Meaningful} {Way} to {Evaluate} {Survival} {Models}},
	url = {http://arxiv.org/abs/2306.01196},
	abstract = {One straightforward metric to evaluate a survival prediction model is based on the Mean Absolute Error (MAE) -- the average of the absolute difference between the time predicted by the model and the true event time, over all subjects. Unfortunately, this is challenging because, in practice, the test set includes (right) censored individuals, meaning we do not know when a censored individual actually experienced the event. In this paper, we explore various metrics to estimate MAE for survival datasets that include (many) censored individuals. Moreover, we introduce a novel and effective approach for generating realistic semi-synthetic survival datasets to facilitate the evaluation of metrics. Our findings, based on the analysis of the semi-synthetic datasets, reveal that our proposed metric (MAE using pseudo-observations) is able to rank models accurately based on their performance, and often closely matches the true MAE -- in particular, is better than several alternative methods.},
	urldate = {2024-04-16},
	publisher = {arXiv},
	author = {Qi, Shi-ang and Kumar, Neeraj and Farrokh, Mahtab and Sun, Weijie and Kuan, Li-Hao and Ranganath, Rajesh and Henao, Ricardo and Greiner, Russell},
	month = jun,
	year = {2023},
	note = {arXiv:2306.01196 [cs, stat]},
	keywords = {Statistics - Machine Learning, Computer Science - Machine Learning, Computer Science - Artificial Intelligence},
	annote = {Comment: Accepted to ICML 2023},
	file = {arXiv Fulltext PDF:/Users/johan/Zotero/storage/XW723V3X/Qi et al. - 2023 - An Effective Meaningful Way to Evaluate Survival M.pdf:application/pdf;arXiv.org Snapshot:/Users/johan/Zotero/storage/9LITXLTM/2306.html:text/html},
}

@article{wilkinson_fair_2016,
	title = {The {FAIR} {Guiding} {Principles} for scientific data management and stewardship},
	volume = {3},
	issn = {2052-4463},
	url = {https://www.nature.com/articles/sdata201618},
	doi = {10.1038/sdata.2016.18},
	abstract = {Abstract
            There is an urgent need to improve the infrastructure supporting the reuse of scholarly data. A diverse set of stakeholders—representing academia, industry, funding agencies, and scholarly publishers—have come together to design and jointly endorse a concise and measureable set of principles that we refer to as the FAIR Data Principles. The intent is that these may act as a guideline for those wishing to enhance the reusability of their data holdings. Distinct from peer initiatives that focus on the human scholar, the FAIR Principles put specific emphasis on enhancing the ability of machines to automatically find and use the data, in addition to supporting its reuse by individuals. This Comment is the first formal publication of the FAIR Principles, and includes the rationale behind them, and some exemplar implementations in the community.},
	language = {en},
	number = {1},
	urldate = {2024-04-20},
	journal = {Scientific Data},
	author = {Wilkinson, Mark D. and Dumontier, Michel and Aalbersberg, IJsbrand Jan and Appleton, Gabrielle and Axton, Myles and Baak, Arie and Blomberg, Niklas and Boiten, Jan-Willem and Da Silva Santos, Luiz Bonino and Bourne, Philip E. and Bouwman, Jildau and Brookes, Anthony J. and Clark, Tim and Crosas, Mercè and Dillo, Ingrid and Dumon, Olivier and Edmunds, Scott and Evelo, Chris T. and Finkers, Richard and Gonzalez-Beltran, Alejandra and Gray, Alasdair J.G. and Groth, Paul and Goble, Carole and Grethe, Jeffrey S. and Heringa, Jaap and ’T Hoen, Peter A.C and Hooft, Rob and Kuhn, Tobias and Kok, Ruben and Kok, Joost and Lusher, Scott J. and Martone, Maryann E. and Mons, Albert and Packer, Abel L. and Persson, Bengt and Rocca-Serra, Philippe and Roos, Marco and Van Schaik, Rene and Sansone, Susanna-Assunta and Schultes, Erik and Sengstag, Thierry and Slater, Ted and Strawn, George and Swertz, Morris A. and Thompson, Mark and Van Der Lei, Johan and Van Mulligen, Erik and Velterop, Jan and Waagmeester, Andra and Wittenburg, Peter and Wolstencroft, Katherine and Zhao, Jun and Mons, Barend},
	month = mar,
	year = {2016},
	pages = {160018},
	file = {Full Text:/Users/johan/Zotero/storage/MSEH3TDF/Wilkinson et al. - 2016 - The FAIR Guiding Principles for scientific data ma.pdf:application/pdf},
}
@article{scikit-learn,
  title={Scikit-learn: Machine Learning in {P}ython},
  author={Pedregosa, F. and Varoquaux, G. and Gramfort, A. and Michel, V.
          and Thirion, B. and Grisel, O. and Blondel, M. and Prettenhofer, P.
          and Weiss, R. and Dubourg, V. and Vanderplas, J. and Passos, A. and
          Cournapeau, D. and Brucher, M. and Perrot, M. and Duchesnay, E.},
  journal={Journal of Machine Learning Research},
  volume={12},
  pages={2825--2830},
  year={2011}
}
@book{joshi_beginning_2016,
	address = {Berkeley, CA},
	title = {Beginning {SOLID} {Principles} and {Design} {Patterns} for {ASP}.{NET} {Developers}},
	copyright = {http://www.springer.com/tdm},
	isbn = {978-1-4842-1847-1 978-1-4842-1848-8},
	url = {http://link.springer.com/10.1007/978-1-4842-1848-8},
	language = {en},
	urldate = {2024-04-28},
	publisher = {Apress},
	author = {Joshi, Bipin},
	year = {2016},
	doi = {10.1007/978-1-4842-1848-8},
}

@misc{haider_effective_2018,
	title = {Effective {Ways} to {Build} and {Evaluate} {Individual} {Survival} {Distributions}},
	url = {http://arxiv.org/abs/1811.11347},
	abstract = {An accurate model of a patient's individual survival distribution can help determine the appropriate treatment for terminal patients. Unfortunately, risk scores (e.g., from Cox Proportional Hazard models) do not provide survival probabilities, single-time probability models (e.g., the Gail model, predicting 5 year probability) only provide for a single time point, and standard Kaplan-Meier survival curves provide only population averages for a large class of patients meaning they are not specific to individual patients. This motivates an alternative class of tools that can learn a model which provides an individual survival distribution which gives survival probabilities across all times - such as extensions to the Cox model, Accelerated Failure Time, an extension to Random Survival Forests, and Multi-Task Logistic Regression. This paper first motivates such "individual survival distribution" (ISD) models, and explains how they differ from standard models. It then discusses ways to evaluate such models - namely Concordance, 1-Calibration, Brier score, and various versions of L1-loss - and then motivates and defines a novel approach "D-Calibration", which determines whether a model's probability estimates are meaningful. We also discuss how these measures differ, and use them to evaluate several ISD prediction tools, over a range of survival datasets.},
	urldate = {2024-04-28},
	publisher = {arXiv},
	author = {Haider, Humza and Hoehn, Bret and Davis, Sarah and Greiner, Russell},
	month = nov,
	year = {2018},
	note = {arXiv:1811.11347 [cs, stat]},
	keywords = {Statistics - Machine Learning, Computer Science - Machine Learning},
	annote = {Comment: 34 pages (main text), 12 figures},
	file = {arXiv Fulltext PDF:/Users/johan/Zotero/storage/XKGTA7ZX/Haider et al. - 2018 - Effective Ways to Build and Evaluate Individual Su.pdf:application/pdf;arXiv.org Snapshot:/Users/johan/Zotero/storage/WLG9L2WL/1811.html:text/html},
}

@article{woo_time_2023,
	title = {Time {Dependence} in the {Cox} {Proportional} {Hazard} {Model} as a {Theory} {Development} {Opportunity}: {A} {Step}-by-{Step} {Guide}},
	issn = {1094-4281, 1552-7425},
	shorttitle = {Time {Dependence} in the {Cox} {Proportional} {Hazard} {Model} as a {Theory} {Development} {Opportunity}},
	url = {http://journals.sagepub.com/doi/10.1177/10944281231205027},
	doi = {10.1177/10944281231205027},
	abstract = {The Cox proportional hazard model has often been used for survival analysis in organizational research. The Cox model needs to satisfy one critical assumption—time independence—that the effects of independent variables are constant over survival time (also known as the proportional hazard assumption). However, organizational research often encounters time dependence in the Cox model. Organizational studies have traditionally seemed to view time dependence as an empirical nuisance, but we highlight that it is also a theory-development opportunity. Indeed, from our review of AMJ and SMJ papers published in a recent 10-year period, we found that researchers rarely considered time dependence as a theory-development opportunity, and worse, many of them did not test for (or report tests for) time dependence. The purpose of our study is to change this pattern. To this end, we provide a step-by-step guide to facilitate testing for time dependence and using time dependence as a theory development opportunity. We also demonstrate our step-by-step guide with an empirical example.},
	language = {en},
	urldate = {2024-04-21},
	journal = {Organizational Research Methods},
	author = {Woo, Hyun-Soo and Kim, Jisun and Cannella, Albert A.},
	month = oct,
	year = {2023},
	pages = {10944281231205027},
}

@misc{davidson-pilon_lifelines_2024,
	title = {lifelines, survival analysis in {Python}},
	copyright = {MIT License},
	url = {https://zenodo.org/doi/10.5281/zenodo.805993},
	abstract = {0.28.0 - 2023-01-03



Fixes bins that are far into the future with using survival\_table\_from\_events, see \#1587

Removed sklean\_adaptor. It was a terrible hack, and causing more confusion and support debt than I want. This cleans up our API and simplifies the library. ✨ There's no replacement, and I doubt I'll introduce one ✨

Fix pandas{\textgreater}=2.0 compatibility.

Fix overflow issue in NelsonAalenfitter, \#1585

officially drop support for {\textless} py3.9

update some dependencies (pandas {\textgreater}= 1.2)},
	urldate = {2024-04-21},
	publisher = {[object Object]},
	author = {Davidson-Pilon, Cameron},
	month = jan,
	year = {2024},
	doi = {10.5281/ZENODO.805993},
	annote = {Other
If you use this software, please cite it using these metadata.},
}

@misc{spath_julianspaethrandom-survival-forest_2021,
	title = {julianspaeth/random-survival-forest: {Create} doi},
	copyright = {Open Access},
	shorttitle = {julianspaeth/random-survival-forest},
	url = {https://zenodo.org/record/5146375},
	abstract = {A Random Survival Forest implementation for python inspired by Ishwaran et al. - Easily understandable, adaptable and extendable.},
	urldate = {2024-04-21},
	publisher = {[object Object]},
	author = {Späth, Julian},
	month = jul,
	year = {2021},
	doi = {10.5281/ZENODO.5146375},
}

@misc{sebastian_polsterl_scikit-survival_2023,
	title = {scikit-survival},
	copyright = {GNU General Public License v3.0 only},
	url = {https://zenodo.org/doi/10.5281/zenodo.3352342},
	abstract = {This release adds support for Python 3.12.

Bug fixes



Fix invalid escape sequence in Introduction of user guide.


Enhancements




Mark Cython functions as noexcept (\#418).



Add support for Python 3.12 (\#422).



Do not use deprecated is\_categorical\_dtype() of Pandas API.



Documentation




Add section Building Cython Code to contributing guidelines (\#379).



Improve the description of the estimate parameter in sksurv.metrics.brier\_score() and sksurv.metrics.integrated\_brier\_score() (\#424).},
	urldate = {2024-04-21},
	publisher = {[object Object]},
	author = {Sebastian Pölsterl},
	month = dec,
	year = {2023},
	doi = {10.5281/ZENODO.3352342},
	keywords = {machine-learning, python, scikit-learn, survival-analysis},
}

@article{kalbfleisch_fifty_2023,
	title = {Fifty {Years} of the {Cox} {Model}},
	volume = {10},
	copyright = {http://creativecommons.org/licenses/by/4.0/},
	issn = {2326-8298, 2326-831X},
	url = {https://www.annualreviews.org/doi/10.1146/annurev-statistics-033021-014043},
	doi = {10.1146/annurev-statistics-033021-014043},
	abstract = {The Cox model is now 50 years old. The seminal paper of Sir David Cox has had an immeasurable impact on the analysis of censored survival data, with applications in many different disciplines. This work has also stimulated much additional research in diverse areas and led to important theoretical and practical advances. These include semiparametric models, nonparametric efficiency, and partial likelihood. In addition to quickly becoming the go-to method for estimating covariate effects, Cox regression has been extended to a vast number of complex data structures, to all of which the central idea of sampling from the set of individuals at risk at time t can be applied. In this article, we review the Cox paper and the evolution of the ideas surrounding it. We then highlight its extensions to competing risks, with attention to models based on cause-specific hazards, and to hazards associated with the subdistribution or cumulative incidence function. We discuss their relative merits and domains of application. The analysis of recurrent events is another major topic of discussion, including an introduction to martingales and complete intensity models as well as the more practical marginal rate models. We include several worked examples to illustrate the main ideas.},
	language = {en},
	number = {1},
	urldate = {2024-04-21},
	journal = {Annual Review of Statistics and Its Application},
	author = {Kalbfleisch, John D. and Schaubel, Douglas E.},
	month = mar,
	year = {2023},
	pages = {1--23},
	file = {Full Text:/Users/johan/Zotero/storage/XELPVAYG/Kalbfleisch and Schaubel - 2023 - Fifty Years of the Cox Model.pdf:application/pdf},
}

@article{freijeirogonzalez_critical_2022,
	title = {A {Critical} {Review} of {LASSO} and {Its} {Derivatives} for {Variable} {Selection} {Under} {Dependence} {Among} {Covariates}},
	volume = {90},
	issn = {0306-7734, 1751-5823},
	url = {https://onlinelibrary.wiley.com/doi/10.1111/insr.12469},
	doi = {10.1111/insr.12469},
	abstract = {Summary
            
              The limitations of the well‐known LASSO regression as a variable selector are tested when there exists dependence structures among covariates. We analyse both the classic situation with
              n
              ≥
              p
              and the high dimensional framework with
              p
               {\textgreater} 
              n
              . Known restrictive properties of this methodology to guarantee optimality, as well as inconveniences in practice, are analysed and tested by means of an extensive simulation study. Examples of these drawbacks are showed making use of different dependence scenarios. In order to search for improvements, a broad comparison with LASSO derivatives and alternatives is carried out. Eventually, we give some guidance about what procedures work best in terms of the considered data nature.},
	language = {en},
	number = {1},
	urldate = {2024-04-21},
	journal = {International Statistical Review},
	author = {Freijeiro‐González, Laura and Febrero‐Bande, Manuel and González‐Manteiga, Wenceslao},
	month = apr,
	year = {2022},
	pages = {118--145},
	file = {Full Text:/Users/johan/Zotero/storage/DB3MWMRA/Freijeiro‐González et al. - 2022 - A Critical Review of LASSO and Its Derivatives for.pdf:application/pdf},
}
